# ğŸ§  Transformer From Scratch (Low-Level, Decoder-Only)

Welcome! This project is a low-level implementation of a **decoder-only Transformer model** built entirely from scratch using **TensorFlow** and **raw Python** no Hugging Face, no Keras shortcuts.

> ğŸ” Built to deeply understand how models like GPT work under the hood â€” from embeddings and attention to training and inference.

---

## ğŸš€ Project Highlights

- âœ… **Manual Multi-Head Self-Attention**
- âœ… **Sinusoidal Positional Encoding**
- âœ… **Custom Tokenizer + Vocabulary** from 4,000+ SQuAD QA pairs
- âœ… **Feedforward + LayerNorm** Implementation
- âœ… **End-to-End Training Pipeline** (with checkpointing, batching)
- âœ… **Inference Loop for Next Token Prediction**
- âœ… **~661,000 Trainable Parameters**

---

## âš ï¸ Heads Up!

> ğŸ§ª **This model uses only a single Transformer layer.**  
> Itâ€™s built for learning, not production use â€” so donâ€™t expect GPT-level responses.  
> The goal here was **understanding**, not state-of-the-art performance.

---

## ğŸ› ï¸ Model Architecture

- **Embedding Layer** â†’ map tokens to 32-dimensional vectors
- **Positional Encoding** â†’ add temporal context
- **Multi-Head Self-Attention** â†’ learn inter-token relationships
- **Feedforward Network** â†’ non-linearity + projection
- **Final Linear Layer** â†’ vocab-sized logits for token prediction

---

## ğŸ§ª Dataset

- Uses the [SQuAD v1.1](https://huggingface.co/datasets/rajpurkar/squad) dataset
- Only the **first 4000 QA pairs** used for building vocabulary and input training sequences
- Each input is tokenized and converted to integer sequences for training

---

## ğŸ“ˆ Training Details

- **Optimizer**: Adam  
- **Loss**: SparseCategoricalCrossentropy (from logits)  
- **Batch Size**: 128  
- **Sequence Length**: 20  
- **Training Epochs**: 100  
- **Device**: RTX 3050 (6GB Laptop GPU)

---

## ğŸ” Transformer Architecture

You can run the inference loop using:
python inference.py
## ğŸ“¸ Screenshot
![Transformer Architecture](https://github.com/artyish/transformer/blob/main/screenshots/diagram.png)

## ğŸ“‚ Folder Structure
```
transformer/
â”œâ”€â”€ attentionhead.py            # Multi-head attention logic
â”œâ”€â”€ forward_network.py          # Feedforward + LayerNorm block
â”œâ”€â”€ train_model.py              # Training loop
â”œâ”€â”€ inference.py                # Inference loop
â”œâ”€â”€ utilities/
â”‚   â”œâ”€â”€ tokenizer.py            # Tokenizer + vocab builder
â”‚   â””â”€â”€ positional_encodings.py # Sinusoidal positional encoding
â”œâ”€â”€ checkpoints/                # TensorFlow checkpoints
```
## ğŸ“œ License
This project is open-source under the MIT License.





